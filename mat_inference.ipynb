{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81295001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracemalloc import start\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from datasets.dataset import *\n",
    "# from models.Autoformer import *(我h'h'h)\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, visual\n",
    "from utils.metrics import metric\n",
    "import os\n",
    "\n",
    "from my_utils import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d32e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "MAST_RATIO = 0.2\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "data_train = Qiang_Anomaly(setting='train', window_size=SEQ_LEN, mask_ratio=MAST_RATIO, test_mode=False, is_abs=False)\n",
    "data_valid = Qiang_Anomaly(setting='valid', window_size=SEQ_LEN, mask_ratio=MAST_RATIO, test_mode=False, is_abs=False)\n",
    "data_test = Qiang_Anomaly(setting='test', window_size=SEQ_LEN, mask_ratio=MAST_RATIO, test_mode=False, is_abs=False)\n",
    "\n",
    "data_train = ConcatDataset([data_train, data_valid])\n",
    "\n",
    "dataloader_train = DataLoader(data_train, batch_size=64, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "dataloader_test = DataLoader(data_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "## Here batch size is equivelant to stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7068519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/602 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(tqdm(dataloader_train)): # (1) stastic on train set\n",
    "    \n",
    "    x, y, index_start, index_end, mask_size, meta = data # bs, channels, seq\n",
    "    \n",
    "    z = torch.concat([x, y], 2)[0].numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7916b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /data/rech/dingqian/data_das/patches_inference.pt\n",
      "Loading picks from: /data/rech/dingqian/data_das/picks_inference.pt\n",
      "Loading indices_start from: /data/rech/dingqian/data_das/index_inference.npy\n",
      "Loading metas from: /data/rech/dingqian/data_das/meta_inference.npy\n"
     ]
    }
   ],
   "source": [
    "dpath = '/data/rech/dingqian/data_das/patches_inference.pt'\n",
    "ppath = '/data/rech/dingqian/data_das/picks_inference.pt'\n",
    "ipath = '/data/rech/dingqian/data_das/index_inference.npy'\n",
    "mpath = '/data/rech/dingqian/data_das/meta_inference.npy'\n",
    "\n",
    "print(f'Loading data from: {dpath}')\n",
    "# img_list = sorted(glob.glob(dpath + '/*.png') + glob.glob(dpath + '/*.jpg'))\n",
    "tensors = torch.load(dpath)\n",
    "# tensors = tensors.unsqueeze(1) # N, 1, 128, 128\n",
    "\n",
    "print(f'Loading picks from: {ppath}')\n",
    "picks = torch.load(ppath)\n",
    "# picks = picks.unsqueeze(1) # N, 1, 128, 128\n",
    "\n",
    "print(f'Loading indices_start from: {ipath}')\n",
    "indices_start = np.load(ipath, allow_pickle=True)\n",
    "\n",
    "print(f'Loading metas from: {mpath}') # [{'file': 'xxx'}]\n",
    "metas = np.load(mpath, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "855d9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading outputs from: /data/rech/dingqian/data_das/output_right_10.npy\n"
     ]
    }
   ],
   "source": [
    "opath = '/data/rech/dingqian/data_das/output_right_10.npy'\n",
    "\n",
    "print(f'Loading outputs from: {opath}') # [{'file': 'xxx'}]\n",
    "outputs = np.load(opath, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c54f4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz(f):\n",
    "    meta = dict(np.load(f))\n",
    "    data = meta[\"data\"]\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    data = moving_average_fast(data.T, 10).T # 900 x 1250\n",
    "\n",
    "    data -= np.median(data, axis=1, keepdims=True)\n",
    "    data -= np.mean(data, axis=0)\n",
    "    data /= np.std(data, axis=0)\n",
    "    data = data.T # 1250 x 900\n",
    "\n",
    "    meta[\"data\"] = data\n",
    "    meta[\"file\"] = f\n",
    "\n",
    "    return meta \n",
    "\n",
    "def map_emd(a, b):\n",
    "    return scipy.stats.wasserstein_distance(a.flatten(), b.flatten())\n",
    "\n",
    "def sliced_emd(a, b, slice_size=32, step_size=16):\n",
    "    a_ts = torch.tensor(a)\n",
    "    b_ts = torch.tensor(b)\n",
    "\n",
    "    a_sliced = a_ts.unfold(0, slice_size, step_size).unfold(1, slice_size, step_size).flatten(0,1)\n",
    "    b_sliced = b_ts.unfold(0, slice_size, step_size).unfold(1, slice_size, step_size).flatten(0,1)\n",
    "\n",
    "    emd_list = list(map(map_emd, a_sliced, b_sliced))\n",
    "    emd = sum(emd_list)\n",
    "\n",
    "    return emd\n",
    "\n",
    "def kl(a, b):\n",
    "    a_norm = scipy.special.softmax(a.flatten())\n",
    "    b_norm = scipy.special.softmax(b.flatten())\n",
    "\n",
    "    ret = 0.5 * (scipy.stats.entropy(a_norm.flatten(), b_norm.flatten()) + scipy.stats.entropy(b_norm.flatten(), a_norm.flatten()))\n",
    "    return ret\n",
    "\n",
    "def js(a, b):\n",
    "    a_norm = scipy.special.softmax(a.flatten())\n",
    "    b_norm = scipy.special.softmax(b.flatten())\n",
    "\n",
    "    ret = scipy.spatial.distance.jensenshannon(a_norm.flatten(), b_norm.flatten())\n",
    "    return ret\n",
    "\n",
    "def hist_kl(a, b):\n",
    "    a_clip = np.clip(a, -2, 2)\n",
    "    b_clip = np.clip(b, -2, 2)\n",
    "\n",
    "    a_hist_prob = np.histogram(a_clip, 10)[0] / np.histogram(a_clip, 10)[0].sum()\n",
    "    b_hist_prob = np.histogram(b_clip, 10)[0] / np.histogram(b_clip, 10)[0].sum()\n",
    "    \n",
    "    ret = 0.5 * (scipy.stats.entropy(a_hist_prob, b_hist_prob) + scipy.stats.entropy(b_hist_prob, a_hist_prob))\n",
    "    return ret\n",
    "\n",
    "def hist_js(a, b):\n",
    "    a_clip = np.clip(a, -2, 2)\n",
    "    b_clip = np.clip(b, -2, 2)\n",
    "\n",
    "    a_hist_prob = np.histogram(a_clip, 10)[0] / np.histogram(a_clip, 10)[0].sum()\n",
    "    b_hist_prob = np.histogram(b_clip, 10)[0] / np.histogram(b_clip, 10)[0].sum()\n",
    "    \n",
    "    ret = scipy.spatial.distance.jensenshannon(a_hist_prob, b_hist_prob)\n",
    "    return ret\n",
    "\n",
    "def get_metrics(y, pred):\n",
    "    ret_auc = roc_auc_score(y, pred)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "    \n",
    "    maxindex = (tpr - fpr).tolist().index(max(tpr - fpr))\n",
    "    threshold = thresholds[maxindex]\n",
    "    \n",
    "    y_pred = pred > threshold\n",
    "    \n",
    "    '''ret_f1 = 0.\n",
    "    for threshold in tqdm(thresholds):\n",
    "        f1_now = f1_score(y, pred > threshold)\n",
    "        if f1_now > ret_f1:\n",
    "            ret_f1 = f1_now\n",
    "            ret_threshold = threshold'''\n",
    "    \n",
    "    ret_threshold = threshold\n",
    "    ret_f1 = f1_score(y, pred > threshold)\n",
    "    \n",
    "    return ret_auc, ret_f1, ret_threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "02fd3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62622/62622 [03:58<00:00, 262.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_DIFF: 0.51635217635449\n",
      "AUC_DIFF_ABS: 0.16063142675802528\n",
      "AUC_EMD: 0.8649358148163835\n",
      "AUC_SUM: 0.5782417695745816\n",
      "AUC_SLICED_EMD: 0.8282211596400559\n",
      "AUC_KL: 0.7240955464208712\n",
      "AUC_JS: 0.7242464931790527\n",
      "AUC_HIST_KL: 0.7061906496383542\n",
      "AUC_HIST_JS: 0.7141182404802505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Tmp/dingqian/miniconda3/envs/ptpy3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Tmp/dingqian/miniconda3/envs/ptpy3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "pred_dict = {}\n",
    "\n",
    "diff_list = []\n",
    "diff_abs_list = []\n",
    "diff_emd_list = []\n",
    "diff_sliced_emd_list = []\n",
    "\n",
    "diff_kl_list = []\n",
    "diff_js_list = []\n",
    "diff_hist_kl_list = []\n",
    "diff_hist_js_list = []\n",
    "\n",
    "sum_list = []\n",
    "\n",
    "has_pick_list = []\n",
    "\n",
    "output_right_list = []\n",
    "output_middle_list = []\n",
    "\n",
    "mask_size = 10\n",
    "mask_start = 128 - 23\n",
    "\n",
    "for i in tqdm(range(len(metas))):\n",
    "    tensor = tensors[i]\n",
    "    pick_np = picks[i].squeeze().numpy()\n",
    "    index_start = indices_start[i]\n",
    "    meta = metas[i]\n",
    "    input_np = tensor.squeeze().numpy()\n",
    "    output_np = outputs[i]\n",
    "    \n",
    "    # print(meta['file'])\n",
    "    \n",
    "    if meta['file'] not in pred_dict:\n",
    "        pred_dict[meta['file']] = load_npz(meta['file'])['data']\n",
    "    \n",
    "    data = pred_dict[meta['file']]\n",
    "    \n",
    "    # patch_origin = data[index_start[0]: index_start[0]+128, index_start[1]: index_start[1]+128]\n",
    "    \n",
    "    x_start = index_start[0] + mask_start\n",
    "    \n",
    "    data[index_start[1]: index_start[1]+128, x_start: x_start + mask_size] = output_np[:, mask_start: mask_start + mask_size]\n",
    "    \n",
    "    pred_dict[meta['file']] = data\n",
    "    \n",
    "    ########### calculate metrics\n",
    "    \n",
    "    input_mask = input_np[:, mask_start: mask_start + mask_size]\n",
    "    output_mask = output_np[:, mask_start: mask_start + mask_size]\n",
    "    pick_mask = pick_np[:, mask_start: mask_start + mask_size]\n",
    "    \n",
    "    has_pick = (pick_mask.sum() > 0)\n",
    "    \n",
    "    diff = (output_mask - input_mask).sum()\n",
    "    diff_abs = (np.abs(output_mask) - np.abs(input_mask)).sum()\n",
    "    diff_emd = scipy.stats.wasserstein_distance(input_mask.flatten(), output_mask.flatten())\n",
    "    diff_sliced_emd = sliced_emd(input_mask, output_mask, 10, 5)\n",
    "\n",
    "    diff_kl = kl(input_mask, output_mask)\n",
    "    diff_js = js(input_mask, output_mask)\n",
    "    diff_hist_kl = hist_kl(input_mask, output_mask)\n",
    "    diff_hist_js = hist_js(input_mask, output_mask)\n",
    "\n",
    "    summ = input_mask.sum()\n",
    "\n",
    "    diff_list.append(diff)\n",
    "    diff_abs_list.append(diff_abs)\n",
    "    diff_emd_list.append(diff_emd)\n",
    "    diff_sliced_emd_list.append(diff_sliced_emd)\n",
    "\n",
    "    diff_kl_list.append(diff_kl)\n",
    "    diff_js_list.append(diff_js)\n",
    "    diff_hist_kl_list.append(diff_hist_kl)\n",
    "    diff_hist_js_list.append(diff_hist_js)\n",
    "\n",
    "    sum_list.append(summ)\n",
    "\n",
    "    has_pick_list.append(has_pick)\n",
    "    \n",
    "    '''print(index_start[1], index_start[1]+128, x_end - 10, x_end)\n",
    "    print(output)\n",
    "    print(data[index_start[1]: index_start[1]+128, index_start[0]: x_end])\n",
    "    print(tensor)'''\n",
    "\n",
    "auc_diff = roc_auc_score(has_pick_list, diff_list)\n",
    "auc_diff_abs = roc_auc_score(has_pick_list, diff_abs_list)\n",
    "auc_emd = roc_auc_score(has_pick_list, diff_emd_list)\n",
    "auc_sum = roc_auc_score(has_pick_list, sum_list)\n",
    "auc_sliced_emd = roc_auc_score(has_pick_list, diff_sliced_emd_list)\n",
    "\n",
    "auc_kl = roc_auc_score(has_pick_list, diff_kl_list)\n",
    "auc_js = roc_auc_score(has_pick_list, diff_js_list)\n",
    "\n",
    "auc_hist_kl = roc_auc_score(has_pick_list, np.nan_to_num(diff_hist_kl_list))\n",
    "auc_hist_js = roc_auc_score(has_pick_list, np.nan_to_num(diff_hist_js_list))\n",
    "\n",
    "print('AUC_DIFF: {}\\nAUC_DIFF_ABS: {}\\nAUC_EMD: {}\\nAUC_SUM: {}\\nAUC_SLICED_EMD: {}'.format(auc_diff, auc_diff_abs, auc_emd, auc_sum, auc_sliced_emd))\n",
    "print('AUC_KL: {}\\nAUC_JS: {}\\nAUC_HIST_KL: {}\\nAUC_HIST_JS: {}'.format(auc_kl, auc_js, auc_hist_kl, auc_hist_js))\n",
    "# roc_auc_score(has_pick_list, diff_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9e3cb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_auc, ret_f1, ret_threshold = get_metrics(has_pick_list, diff_sliced_emd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "69ec3dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8282211596400559, 0.19660207966361726, 5.569320248818258)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_auc, ret_f1, ret_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0bdad9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62622/62622 [00:06<00:00, 10101.41it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_dict = {}\n",
    "heat_dict = {}\n",
    "mask_size = 10\n",
    "mask_start = 128 - 23\n",
    "\n",
    "for i in tqdm(range(len(metas))):\n",
    "    tensor = tensors[i]\n",
    "    pick_np = picks[i].squeeze().numpy()\n",
    "    index_start = indices_start[i]\n",
    "    meta = metas[i]\n",
    "    input_np = tensor.squeeze().numpy()\n",
    "    output_np = outputs[i]\n",
    "    \n",
    "    ### diff_emd_list should be replaced\n",
    "    \n",
    "    # pred_label = diff_emd_list[i] > (0.1)\n",
    "    #pred_label = diff_sliced_emd_list[i] > (ret_threshold)\n",
    "    \n",
    "    pred_label = diff_emd_list[i]\n",
    "    \n",
    "    # print(meta['file'])\n",
    "    \n",
    "    #### Load pred, heat\n",
    "    if meta['file'] not in pred_dict:\n",
    "        pred_dict[meta['file']] = load_npz(meta['file'])['data']\n",
    "        heat_dict[meta['file']] = np.zeros(pred_dict[meta['file']].shape)\n",
    "    \n",
    "    data = pred_dict[meta['file']]\n",
    "    heat = heat_dict[meta['file']]\n",
    "    \n",
    "    # patch_origin = data[index_start[0]: index_start[0]+128, index_start[1]: index_start[1]+128]\n",
    "    \n",
    "    #### Calculate new data and new heat based on threshold\n",
    "    x_start = index_start[0] + mask_start\n",
    "    \n",
    "    data[index_start[1]: index_start[1]+128, x_start: x_start + mask_size] = output_np[:, mask_start: mask_start + mask_size]\n",
    "    \n",
    "    heat[index_start[1]: index_start[1]+128, x_start: x_start + mask_size] = pred_label\n",
    "    \n",
    "    pred_dict[meta['file']] = data\n",
    "    heat_dict[meta['file']] = heat\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "69c7aa04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  333],\n",
       "       [   0,  334],\n",
       "       [   0,  335],\n",
       "       ...,\n",
       "       [1215,  880],\n",
       "       [1215,  881],\n",
       "       [1215,  882]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = np.where(heat == 1)\n",
    "np.stack([x,y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8f4a6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_z(z, file_name, heat=None): # N x M\n",
    "    plt.figure(figsize=(8,8), frameon=False)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(z, aspect='auto', vmin=-2.0, vmax=2.0, cmap=\"seismic\")\n",
    "    \n",
    "    heat = heat / heat.max()\n",
    "    plt.imshow(heat, aspect='auto', vmin=0., vmax=1.0, cmap=\"seismic\", alpha=0.8)\n",
    "    ### Add heat\n",
    "    '''if heat is not None:\n",
    "        x, y = np.where(heat > 0)\n",
    "        # np.stack([x,y], axis=1)\n",
    "        # plt.plot(y, x, alpha=0.5, color='r')\n",
    "        alpha = 1. if heat[x, y] > 2. else heat[x, y]\n",
    "        plt.plot(y, x, alpha=heat[x, y], color='r')'''\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join('./samples/inference_10', os.path.split(file_name)[-1] + '.jpg'), pad_inches=0, dpi=64)\n",
    "    \n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e9c5ee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DAS/test_npz/2020-12-01T01-54-49-M3-24km.npz\n",
      "./data/DAS/test_npz/2021-05-27T15-18-26-M3-30km.npz\n",
      "./data/DAS/test_npz/2020-07-25T19-36-01-M2-15km.npz\n",
      "./data/DAS/test_npz/2020-04-05T08-21-41-M2-55km.npz\n",
      "./data/DAS/test_npz/2020-04-04T04-19-09-M2-17km.npz\n",
      "./data/DAS/test_npz/2019-11-10T12-35-18-M2-24km.npz\n",
      "./data/DAS/test_npz/2019-08-18T22-02-28-M3-22km.npz\n",
      "./data/DAS/test_npz/2020-10-02T06-00-56-M2-24km.npz\n",
      "./data/DAS/test_npz/2019-12-08T09-59-23-M1-12km.npz\n",
      "./data/DAS/test_npz/2021-02-20T09-54-10-M2-33km.npz\n",
      "./data/DAS/test_npz/2019-08-25T10-36-27-M3-19km.npz\n",
      "./data/DAS/test_npz/2019-09-03T10-36-23-M2-22km.npz\n",
      "./data/DAS/test_npz/2019-09-17T05-01-20-M2-16km.npz\n",
      "./data/DAS/test_npz/2021-04-08T20-51-15-M4-15km.npz\n",
      "./data/DAS/test_npz/2019-11-07T06-06-05-M2-17km.npz\n",
      "./data/DAS/test_npz/2019-12-03T04-41-40-M2-19km.npz\n",
      "./data/DAS/test_npz/2020-05-14T01-36-08-M2-19km.npz\n",
      "./data/DAS/test_npz/2020-04-10T04-55-32-M2-18km.npz\n",
      "./data/DAS/test_npz/2020-04-22T10-21-56-M2-18km.npz\n",
      "./data/DAS/test_npz/2019-12-19T09-12-59-M2-16km.npz\n",
      "./data/DAS/test_npz/2020-10-12T11-52-01-M3-15km.npz\n",
      "./data/DAS/test_npz/2020-03-07T12-51-16-M4-56km.npz\n",
      "./data/DAS/test_npz/2020-01-23T04-27-32-M2-22km.npz\n",
      "./data/DAS/test_npz/2020-06-04T03-47-35-M3-29km.npz\n",
      "./data/DAS/test_npz/2020-06-23T00-25-46-M4-95km.npz\n",
      "./data/DAS/test_npz/2019-08-23T23-14-22-M3-13km.npz\n",
      "./data/DAS/test_npz/2020-09-05T03-14-58-M3-24km.npz\n",
      "./data/DAS/test_npz/2019-08-24T11-37-27-M2-27km.npz\n",
      "./data/DAS/test_npz/2021-04-15T18-05-03-M3-16km.npz\n",
      "./data/DAS/test_npz/2020-04-02T13-07-27-M2-14km.npz\n",
      "./data/DAS/test_npz/2019-11-03T15-45-55-M3-27km.npz\n",
      "./data/DAS/test_npz/2019-09-01T13-01-26-M2-27km.npz\n",
      "./data/DAS/test_npz/2019-11-20T22-51-10-M3-19km.npz\n",
      "./data/DAS/test_npz/2021-03-31T03-39-09-M2-15km.npz\n",
      "./data/DAS/test_npz/2020-03-01T11-59-39-M2-21km.npz\n",
      "./data/DAS/test_npz/2019-09-03T02-32-14-M3-18km.npz\n",
      "./data/DAS/test_npz/2019-11-02T07-38-42-M2-14km.npz\n",
      "./data/DAS/test_npz/2020-11-18T08-47-08-M2-24km.npz\n",
      "./data/DAS/test_npz/2021-04-11T09-45-36-M3-17km.npz\n",
      "./data/DAS/test_npz/2020-03-27T01-34-30-M2-14km.npz\n",
      "./data/DAS/test_npz/2019-10-31T08-49-14-M3-19km.npz\n",
      "./data/DAS/test_npz/2019-08-20T17-33-30-M2-14km.npz\n",
      "./data/DAS/test_npz/2021-04-24T06-12-20-M2-16km.npz\n",
      "./data/DAS/test_npz/2019-09-28T18-08-37-M2-17km.npz\n",
      "./data/DAS/test_npz/2019-12-07T20-34-15-M2-21km.npz\n"
     ]
    }
   ],
   "source": [
    "for file_name, data_pred in pred_dict.items():\n",
    "    print(file_name)\n",
    "    data = load_npz(file_name)['data']\n",
    "    heat = heat_dict[file_name]\n",
    "    \n",
    "    save_z(data_pred, file_name + '_pred', heat)\n",
    "    save_z(data, file_name, heat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "72879376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6707442508025903"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(diff_emd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2159bdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3843120070113935"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4d93a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Test \n",
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "def get_fft(z):\n",
    "    n = len(z)\n",
    "    yf = rfft(z)\n",
    "    xf = rfftfreq(n, 0.1)\n",
    "    return np.abs(yf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b5e5e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outputs[0]\n",
    "inp = tensors[0].squeeze().numpy()\n",
    "\n",
    "output_f = np.zeros((128, 65), dtype=float)\n",
    "for i in range(128):\n",
    "    output_f[i] = get_fft(output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "cfc5a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 640x480 with 1 Axes>\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(output_f, aspect='auto', vmin=-5.0, vmax=5.0, cmap=\"seismic\")\n",
    "plt.savefig('freq_output.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "209be281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 640x480 with 1 Axes>\n"
     ]
    }
   ],
   "source": [
    "input_f = np.zeros((128, 65), dtype=float)\n",
    "for i in range(128):\n",
    "    input_f[i] = get_fft(inp[i])\n",
    "\n",
    "plt.imshow(input_f, aspect='auto', vmin=-5.0, vmax=5.0, cmap=\"seismic\")\n",
    "plt.savefig('freq_input.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f17c2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 640x480 with 1 Axes>\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(output_f - input_f, aspect='auto', vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "plt.savefig('freq_diff.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "63e1d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1548734 , -0.24163167,  0.21370491, ..., -0.7274417 ,\n",
       "        -0.22254851, -0.5854341 ],\n",
       "       [ 1.6204568 , -0.7617863 ,  0.13278896, ..., -0.03637507,\n",
       "         0.67147183, -0.13801157],\n",
       "       [ 1.0656402 , -1.6293908 ,  1.0746189 , ..., -1.6335528 ,\n",
       "         0.40482336,  1.041287  ],\n",
       "       ...,\n",
       "       [-0.14853552, -0.04680762,  0.27739307, ...,  0.00496372,\n",
       "        -0.10391421, -0.03731848],\n",
       "       [-0.27020475,  0.2362417 ,  0.43385735, ..., -0.08310726,\n",
       "         0.08506312, -0.26480883],\n",
       "       [ 0.23798843,  0.04592361, -0.4435884 , ...,  0.06911898,\n",
       "         0.0487297 , -0.02855118]], dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[0].squeeze().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptpy3] *",
   "language": "python",
   "name": "conda-env-ptpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
